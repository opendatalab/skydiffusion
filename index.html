<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="./static/image/SkyDiffusion.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="./static/images/SkyDiffusion.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SkyDiffusion</title>
  <link rel="icon" type="image/x-icon" href="./static/images/SkyDiffusion.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/background-video.css">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.10.377/pdf.min.js"></script>


  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero" id="first-section">
  <div  id="video-div" style="height: 100%; width: 100%; position: relative;">
    <!-- ËÉåÊôØËßÜÈ¢ë -->
    <video id="background-video" autoplay muted loop class="background-video">
      <source src="./static/videos/1.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <!-- <div class="overlay"></div> -->

    <h1 class="title is-1 publication-title" style="margin-top: 80px;">
      <img id="painting_icon1" width="8%" src="./static/icons/skydiffusion.png"
        style="vertical-align: middle; margin-right: 0px; position: relative; top: -8px">
      <span class="highlight">SkyDiffusion</span>
    </h1>
    <h1 class="title publication-title" id="title-full">Ground-to-Aerial Image Synthesis <br> with Diffusion Models <br> and BEV Paradigm</h1>
    <br>
    <!-- <div class="is-size-5 publication-authors special-authors">
        <span class="author-block">
          <a href="https://yejy53.github.io/" target="_blank">Junyan Ye</a><sup>1,2</sup>,</span>
        </span>
        <span class="author-block">
          <a href="" target="_blank">Jun He</a><sup>1</sup>,</span>
        <span class="author-block">
          <a href="https://liweijia.github.io/" target="_blank">Weijia Li</a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>‚Ä†</mo></math></sup>,</span>
        <span class="author-block">
          <a href="" target="_blank">Zhutao Lv</a><sup>1</sup>,</span>
        </span>
        <br>
        <span class="author-block">
          <a href="" target="_blank">Jinhua Yu</a><sup>1</sup>,</span>
        </span>
        <span class="author-block">
          <a href="" target="_blank">Haote Yang</a><sup>2</sup>,</span>
        </span>
        <span class="author-block">
          <a href="https://conghui.github.io/" target="_blank">Conghui He</a><sup>2,3 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>‚Ä†</mo></math></sup></span>
        </span>
    </div>

    <div class="is-size-5 publication-authors institutions">
      <span class="author-block"><sup>1</sup>Sun Yat-Sen University </span>
      <span class="author-block"><sup>2</sup>Shanghai AI Laboratory </span>
      <span class="author-block"><sup>3</sup>SenseTime Research</span>
    </div> -->
    <div class="publication-authors-container special-authors">
      <div class="authors">
        <div class="author-line">
          <span class="author-block"><a href="https://yejy53.github.io/" target="_blank">Junyan Ye</a><sup>1,2</sup></span>,
          <span class="author-block"><a href="" target="_blank">Jun He</a><sup>1</sup></span>,
          <span class="author-block"><a href="https://liweijia.github.io/" target="_blank">Weijia Li</a><sup>1<math xmlns="http://www.w3.org/1998/Math/MathML"><mo>‚Ä†</mo></math></sup></span>,
          <span class="author-block"><a href="" target="_blank">Zhutao Lv</a><sup>1</sup></span>
        </div>
        <div class="author-line">
          <span class="author-block"><a href="" target="_blank">Jinhua Yu</a><sup>1</sup></span>,
          <span class="author-block"><a href="" target="_blank">Haote Yang</a><sup>2</sup></span>,
          <span class="author-block"><a href="https://conghui.github.io/" target="_blank">Conghui He</a><sup>2,3 <math xmlns="http://www.w3.org/1998/Math/MathML"><mo>‚Ä†</mo></math></sup></span>
        </div>
      </div>
    
      <div class="divider"></div>
    
      <div class="institutions">
        <span class="institution-block"><sup>1</sup>Sun Yat-Sen University</span>
        <span class="institution-block"><sup>2</sup>Shanghai AI Laboratory</span>
        <span class="institution-block"><sup>3</sup>SenseTime Research</span>
      </div>
    </div>
    <div class="publication-authors-container special-authors-second"></div>
    <!-- <div class="publication-authors-container special-authors-github"></div> -->
    <br><br>
    <div class="publication-authors-container special-link">
      <div class="links">
        <span class="link-block"><a href="https://arxiv.org/abs/2408.01812" target="_blank">üìÑ Paper </a> <span style="color:black">/</span></span>
        <span class="link-block"><a href="https://github.com/opendatalab/skydiffusion" target="_blank"><svg class="svg-inline--fa fa-github fa-w-16" style="color:black" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg> Code</a><span style="color:black"> /</span></span>
        <span class="link-block"><a href="https://huggingface.co/datasets/Yejy53/G2A-3" target="_blank"> ü§ó Dataset</a></span>
      </div>
    </div>
    <div class="publication-authors-container special-link-second"></div>

      <!-- ‰∏≠Èó¥ÊèèËø∞ÊñáÂ≠ó -->
      <div class="video-caption-middle">
        <h2 class="video-middle-h2">
          <div class="video-middle-content">
            <!-- <img class="video-middle-icon" src="./static/icons/Task.png" style="transform: translateX(12%);"> -->
            <span class="video-middle-text">
              <span class="video-title">Task Description</span> <br> 
              Ground-to-aerial image synthesis focuses on generating realistic aerial images from corresponding ground street view
              images while maintaining consistent content layout, simulating a top-down view.
            </span>
            <img class="video-middle-icon" src="./static/icons/Task.png" style="transform: translateX(12%);">
          </div>
        </h2>

        <h2 class="video-middle-h2">
          <div class="video-middle-content">
            <!-- <img class="video-middle-icon" src="./static/icons/Challenge.png"> -->
            <span class="video-middle-text">
              <span class="video-title">Challenge</span> <br>
              The significant viewpoint difference
              leads to domain gaps between views, and dense urban scenes limit the visible range of street views, making this
              cross-view generation task particularly challenging.
            </span>
            <img class="video-middle-icon" src="./static/icons/Challenge.png">
          </div>
        </h2>

        <h2 class="video-middle-h2">
          <div class="video-middle-content">
            <!-- <img class="video-middle-icon" src="./static/icons/Method.png"> -->
            <span class="video-middle-text">
              <span class="video-title">Method</span><br>
              We introduce SkyDiffusion, a novel cross-view generation method for synthesizing aerial images from
              street view images, utilizing a diffusion model and the Bird‚Äôs-Eye View (BEV) paradigm. 
            </span>
            <img class="video-middle-icon" src="./static/icons/Method.png">
          </div>
        </h2>

        <h2 class="video-middle-h2">
          <div class="video-middle-content">
            <!-- <img class="video-middle-icon" src="./static/icons/NewDataset.png"> -->
            <span class="video-middle-text">
              <span class="video-title">New Dataset</span> <br>
              We introduce a novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial
              image synthesis
              applications, including disaster scene aerial synthesis, historical high-resolution satellite image synthesis, and
              low-altitude UAV image synthesis tasks.
            </span>
            <img class="video-middle-icon" src="./static/icons/NewDataset.png">
          </div>
        </h2>


      </div>

    
      <!-- Âè≥‰∏ãËßíÊèèËø∞ÊñáÂ≠ó -->
      <div class="video-caption-rightbottom">
        The video shows the result of pulling up from the ground street view to the aerial perspective, like a sky-down
        perspective. The video comes from Google Engine rendering and MatrixCity rendering
      </div>

    <!-- Â∑¶Âè≥ÊéßÂà∂ÊåâÈíÆ -->
    <div class="video-controls">
      <!-- ‰∏ä‰∏ÄËßÜÈ¢ëÊåâÈíÆ -->
      <button id="prev-video" class="control-button">‚óÄ</button>
        <div class="video-thumbnails">
        <div class="thumbnail-item" id="thumbnail-1">
          <video class="thumbnail-video" muted loop autoplay>
            <source src="./static/videos/1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="thumbnail-item" id="thumbnail-2">
          <video class="thumbnail-video" muted loop autoplay>
            <source src="./static/videos/2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <!-- ‰∏ã‰∏ÄËßÜÈ¢ëÊåâÈíÆ -->
      <button id="next-video" class="control-button">‚ñ∂</button>
    </div>
  </div>
</section>


    <!-- Synthesis results -->
    <section class="section hero">
      <h2
        class="title is-3"
        style="
          width: 40%;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        <img
          src="./static/icons/skydiffusion.png"
          alt="Icon"
          style="
            width: 50px;
            height: 50px;
            vertical-align: middle;
            margin-right: 10px;
          "
        />Ground-to-Aerial Image Synthesis results.
      </h2>
      <h1
        style="
          width: 40%;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin: 20px auto;
        "
      >
        SkyDiffusion outperforms state-of-the-art methods on cross-view datasets
        across natural (CVUSA), suburban (CVACT), urban (VIGOR-Chicago), and
        various application scenarios (G2A-3), generating realistic, consistent
        aerial images.
      </h1>
      <div id="Tabdiv">
        <!-- Ê†áÁ≠æË°å -->
        <div class="tab-row">
          <div class="tab-item" id="tab-1" data-tab-name="CVACT">
            <img src="./static/icons/CVACT.PNG" alt="CVACT" class="tab-icon" />
            CVACT
          </div>
          <div class="tab-item" id="tab-2" data-tab-name="CVUSA">
            <img src="./static/icons/CVUSA.PNG" alt="CVUSA" class="tab-icon" />
            CVUSA
          </div>
          <div
            class="tab-item"
            id="tab-3"
            data-tab-name="Vigor"
            style="gap: 22px"
          >
            <img src="./static/icons/Vigor.PNG" alt="Vigor" class="tab-icon" />
            Vigor
          </div>
          <div class="tab-item" id="tab-4" data-tab-name="G2A-3">
            <img src="./static/icons/G2A-3.png" alt="G2A-3" class="tab-icon" />
            G2A-3
          </div>
          <!-- Ê†áÁ≠æÊåáÁ§∫Âô® -->
          <div class="tab-indicator"></div>
        </div>
      </div>
      <!-- ÊòæÁ§∫ÂõæÁâá -->
      <div id="images-container" class="images-container">
        <button id="prev-btn" class="nav-btn">‚óÄ</button>
        <canvas id="pdf-canvas" class="dataset-pdf"></canvas>
        <button id="next-btn" class="nav-btn">‚ñ∂</button>
      </div>
      <div id="img-indicator" class="img-indicator"></div>
    </section>

    <!-- End Synthesis results -->

    <!-- Dataset -->
    <section class="section hero">
      <h2
        class="title is-3"
        style="
          width: 40%;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        <img
          src="./static/icons/title_dataset.png"
          alt="Icon"
          style="
            width: 50px;
            height: 50px;
            vertical-align: middle;
            margin-right: 10px;
          "
        />
        Ground2Aerial-3 Dataset
      </h2>
      <div
        class="Ground2Aerial-3"
        style="
          display: flex;
          flex-direction: column;
          align-items: center;
          gap: 20px;
        "
      >
        <h1
          style="
            width: 40%;
            font-size: 20px;
            line-height: 1.5;
            text-align: justify;
            margin-bottom: 30px;
          "
        >
          We propose Ground2Aerial-3, a multi-task cross-view synthesis dataset
          designed to explore the performance of cross-view synthesis methods in
          several novel scenarios. As shown in Figure 3, G2A-3 contains nearly
          10k pairs of street-view and aerial images, covering disaster scene
          aerial image synthesis, historical high-resolution satellite image
          synthesis, and low-altitude UAV image synthesis. The dataset of each
          task is randomly split into training and test sets with a ratio of
          5:1. The ground street-view images are 1024√ó512, with true north
          aligned at the center, and the aerial images are 512√ó512, aligned with
          the center of the ground images.
        </h1>

        <div
          style="
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 5%;
            margin-bottom: 30px;
          "
        >
          <img
            src="./static/images/Ground2Aerial-3_Dataset.jpg"
            style="height: 25vh; object-fit: contain"
          />
          <img
            src="./static/images/MatrixCity.png"
            style="height: 25vh; object-fit: contain"
          />
        </div>

        <h1
          style="
            width: 40%;
            font-size: 20px;
            line-height: 1.5;
            text-align: justify;
            margin-bottom: 30px;
          "
        >
          For the low-altitude UAV image synthesis task, we used the virtual
          MatrixCity environment and the UE engine to position six single-view
          cameras at ground level with different perspectives. These include
          four horizontal perspectives, one upward, and one downward view. The
          six single-view images are initially rendered and then stitched into
          panoramic data using the py360convert library. Simultaneously, at the
          same xy position but at an altitude of 20m, a UAV is simulated with a
          downward-facing camera to generate UAV data, forming a cross-view
          dataset.
        </h1>

        <div class="datasetsVideos" style="margin-bottom: 30px">
          <video
            src="./static/datasets/aerial_video_h264.mp4"
            autoplay
            controls
            loop
            style="height: 25vh"
          ></video>
          <video
            src="./static/datasets/pano_video_h264.mp4"
            autoplay
            controls
            loop
            style="height: 25vh"
          ></video>
        </div>

        <h1
          style="
            width: 40%;
            font-size: 20px;
            line-height: 1.5;
            text-align: justify;
            margin-bottom: 30px;
          "
        >
          The images below show sample visualizations of data from different
          tasks.
        </h1>

        <div class="datasetsImages" style="margin-bottom: 30px">
          <button id="datasets-prev-btn" class="datasets-nav-btn">‚óÄ</button>
          <canvas id="datasets-canvas" class="datasets-pdf"></canvas>
          <button id="datasets-next-btn" class="datasets-nav-btn">‚ñ∂</button>
          <div class="datasets-pagination"></div>
        </div>
      </div>
    </section>

    <!-- End Dataset -->

    <!-- Method -->
    <section
      class="section hero is-light"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40%;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        <img
          src="./static/icons/title_method.png"
          alt="Icon"
          style="
            width: 50px;
            height: 50px;
            vertical-align: middle;
            margin-right: 10px;
          "
        />Method
      </h2>
      <img
        src="./static/images/Method.jpg"
        alt="Interpolate start reference image."
        style="width: 40vw; object-fit: contain"
      />
      <figcaption
        style="
          font-size: 14px;
          text-align: center;
          margin-top: 20px;
          width: 40%;
        "
      >
        <span style="font-weight: bold"
          >Overview of the proposed SkyDiffusion framework</span
        >, including the curved BEV transformation and BEV-controlled diffusion
        model with light manipulation. The lower parts present the results of
        One-to-One and Multi-to-One BEV transformations, respectively.
      </figcaption>
    </section>

    <!-- End method -->

    <!-- Evaluation -->
    <section
      class="section hero"
      style="
        display: flex;
        flex-direction: column;
        align-items: center;
        text-align: center;
      "
    >
      <h2
        class="title is-3"
        style="
          width: 40%;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        <img
          src="./static/icons/title_evaluation.png"
          alt="Icon"
          style="
            width: 50px;
            height: 50px;
            vertical-align: middle;
            margin-right: 10px;
          "
        />Evaluation
      </h2>
      <h3
        style="
          width: 40%;
          font-weight: bold;
          font-size: 20px;
          text-align: left;
          margin-bottom: 20px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        Quantitative Evaluation of Existing datasets
      </h3>

      <h1
        style="
          width: 40%;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
        "
      >
        On the suburban CVUSA and CVACT datasets, our SkyDiffusion method
        achieved the outstanding results. Compared to state-of-the-art methods,
        it reduced FID by <b>25.72%</b> and increased SSIM by <b>7.68%</b>,
        demonstrating its superiority in synthesizing realistic and consistent
        satellite images. In the urban VIGOR-Chicago dataset, SkyDiffusion
        reduced FID by <b>14.9%</b> and improved SSIM by <b>9.41%</b> compared
        to the state-of-the-art method.
      </h1>

      <img
        src="./static/images/main1.png"
        class="interpolation-image"
        alt="Interpolate start reference image."
        style="width: 40vw; height: auto"
      />

      <h1
        style="
          width: 40%;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
        "
      >
        The tasks on the G2A-3 dataset present certain challenges; however, our
        method achieves significant performance improvements over the commonly
        used image-conditioned synthesis method, ControlNet. SkyDiffusion
        reduces the FID by an average of <b>19.60%</b> and increases the SSIM by
        an average of <b>9.90%</b>.
      </h1>
      <div
        style="
          display: flex;
          justify-content: center;
          align-items: center;
          gap: 20px;
        "
      >
        <img
          src="./static/images/main2.png"
          alt="Interpolate start reference image."
          style="width: 19vw; height: auto"
        />
        <img
          src="./static/images/main1_2.png"
          alt="Interpolate start reference image."
          style="width: 19vw; height: auto"
        />
      </div>

      <div style="display: flex; flex-direction: column; align-items: center">
        <h1
          style="
            width: 40vw;
            font-size: 20px;
            line-height: 1.5;
            text-align: justify;
            margin-bottom: 30px;
          "
        >
          We conducted ablation experiments on the datasets. Compared to
          directly using street-view images as input, the Curved-BEV method
          improves performance across multiple metrics by transforming
          street-view images into satellite views for domain alignment. This
          indicates that the Curved-BEV method aids in synthesizing more
          content-consistent satellite images. Furthermore, the Multi-to-one
          method further improves metrics compared to the one-to-one mapping,
          demonstrating its effectiveness in dense urban scenes.
        </h1>

        <h1
          style="
            width: 40vw;
            font-size: 20px;
            line-height: 1.5;
            text-align: justify;
            margin-bottom: 30px;
          "
        >
          We applied an optional Light Manipulation module to align the lighting
          conditions of the synthesized image with those of the target domain
          image. The results indicate that Light Manipulation can effectively
          improve SSIM and PSNR metrics. This module preserves the original
          image content while providing more flexible lighting conditions for
          the synthesized satellite image.
        </h1>
      </div>
    </section>

    <!-- End evaluation -->

    <!-- Conclusion -->
    <section class="section hero is-light">
      <h2
        class="title is-3"
        style="
          width: 40%;
          text-align: left;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        <img
          src="./static/icons/title_summarise.png"
          alt="Icon"
          style="
            width: 50px;
            height: 50px;
            vertical-align: middle;
            margin-right: 10px;
          "
        />
        Conclusion
      </h2>
      <h1
        style="
          width: 40%;
          font-size: 20px;
          line-height: 1.5;
          text-align: justify;
          margin-bottom: 30px;
          margin-left: auto;
          margin-right: auto;
        "
      >
        In this study, we introduce SkyDiffusion, a novel approach specifically
        designed for street images to satellite images cross-view synthesis.
        SkyDiffusion operates solely with street images as input, utilizing a
        BEV Paradigm and diffusion models to generate satellite images.
        SkyDiffusion achieves state-of-the-art performance in both content
        consistency and image realism on across multiple cross-view datasets,
        demonstrating its superior capabilities. Additionally, we introduce a
        cross-view synthesis dataset, Ground2Aerial-3, featuring aerial image
        synthesis tasks for multiple new scenes, providing practical value and
        inspiration for future cross-view synthesis research.
      </h1>
    </section>

    <!-- End Conclusion -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the¬†<a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                >¬†project page. You are free to borrow the of this website, we
                just ask that you link back to this page in the footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
    <script>
      const videos = ["./static/videos/1.mp4", "./static/videos/2.mp4"];
      let currentVideoIndex = 0;

      const videoElement = document.getElementById("background-video");
      const prevButton = document.getElementById("prev-video");
      const nextButton = document.getElementById("next-video");
      const videoThumbnails = document.querySelectorAll(".thumbnail-item");

      function updateVideoAndThumbnails() {
        // Êõ¥Êñ∞ËÉåÊôØËßÜÈ¢ë
        videoElement.src = videos[currentVideoIndex];
        videoElement.play();

        // Êõ¥Êñ∞Áº©Áï•Âõæ
        videoThumbnails.forEach((thumbnail, index) => {
          if (index === currentVideoIndex) {
            thumbnail.classList.add("active");
            thumbnail.classList.remove("inactive");
          } else {
            thumbnail.classList.remove("active");
            thumbnail.classList.add("inactive");
          }
        });
      }

      // ÁÇπÂáªÁº©Áï•ÂõæÂàáÊç¢ËßÜÈ¢ë
      videoThumbnails.forEach((thumbnail, index) => {
        thumbnail.addEventListener("click", () => {
          currentVideoIndex = index;
          updateVideoAndThumbnails();
        });
      });

      // ÂàáÊç¢Âà∞‰∏ä‰∏ÄËßÜÈ¢ë
      prevButton.addEventListener("click", () => {
        currentVideoIndex =
          (currentVideoIndex - 1 + videos.length) % videos.length;
        updateVideoAndThumbnails();
      });

      // ÂàáÊç¢Âà∞‰∏ã‰∏ÄËßÜÈ¢ë
      nextButton.addEventListener("click", () => {
        currentVideoIndex = (currentVideoIndex + 1) % videos.length;
        updateVideoAndThumbnails();
      });

      // ÂàùÂßãÂåñ
      updateVideoAndThumbnails();

      // Ëé∑ÂèñËßÜÈ¢ëÂÖÉÁ¥†
      const backgroundVideo = document.getElementById("background-video");
      const thumbnailVideos = document.querySelectorAll(".thumbnail-video");

      // ÂêåÊ≠•Êí≠ÊîæÂíåÊöÇÂÅúÁä∂ÊÄÅ
      backgroundVideo.addEventListener("play", () => {
        thumbnailVideos.forEach((video) => video.play());
      });

      backgroundVideo.addEventListener("pause", () => {
        thumbnailVideos.forEach((video) => video.pause());
      });

      // ÂêåÊ≠•ËßÜÈ¢ëÊó∂Èó¥
      backgroundVideo.addEventListener("timeupdate", () => {
        thumbnailVideos.forEach((video) => {
          if (Math.abs(video.currentTime - backgroundVideo.currentTime) > 0.1) {
            video.currentTime = backgroundVideo.currentTime;
          }
        });
      });

      // ÂêåÊ≠•ËÉåÊôØËßÜÈ¢ëÊó∂Èó¥Êó∂ËÆæÁΩÆËäÇÊµÅÔºàÂèØÈÄâÔºåÁî®‰∫éÊÄßËÉΩ‰ºòÂåñÔºâ
      let syncTimeout;
      backgroundVideo.addEventListener("seeking", () => {
        clearTimeout(syncTimeout);
        syncTimeout = setTimeout(() => {
          thumbnailVideos.forEach((video) => {
            video.currentTime = backgroundVideo.currentTime;
          });
        }, 50);
      });
    </script>

    <script>
      pdfjsLib.GlobalWorkerOptions.workerSrc =
        "https://cdnjs.cloudflare.com/ajax/libs/pdf.js/2.10.377/pdf.worker.min.js";

      const resultsPDFs = {
        CVACT: [
          "./static/results/CVACT1.pdf",
          "./static/results/CVACT2.pdf",
          "./static/results/CVACT3.pdf",
        ],
        CVUSA: [
          "./static/results/CVUSA1.pdf",
          "./static/results/CVUSA2.pdf",
          "./static/results/CVUSA3.pdf",
        ],
        Vigor: [
          "./static/results/vigor1.pdf",
          "./static/results/vigor2.pdf",
          "./static/results/vigor3.pdf",
        ],
        "G2A-3": [
          "./static/results/UAV.pdf",
          "./static/results/History.pdf",
          "./static/results/disater.pdf",
        ],
      };

      let currentImageIndex = 0;

      // ÂºÇÊ≠•Âä†ËΩΩ PDF Âπ∂Ê∏≤ÊüìÂà∞ Canvas
      function loadPDFAsync(src) {
        const canvas = document.getElementById("pdf-canvas");
        const context = canvas.getContext("2d");

        pdfjsLib.getDocument(src).promise.then((pdf) => {
          // Ëé∑ÂèñÁ¨¨‰∏ÄÈ°µ
          pdf.getPage(1).then((page) => {
            const viewport = page.getViewport({ scale: 1.5 });
            canvas.width = viewport.width;
            canvas.height = viewport.height;
            const renderContext = {
              canvasContext: context,
              viewport: viewport,
            };
            page.render(renderContext);
          });
        });
      }

      function updatePDF(tab) {
        const pdfs = resultsPDFs[tab];
        const pdfSrc = pdfs[currentImageIndex];
        loadPDFAsync(pdfSrc);
        updateImageIndicators();
      }

      function switchPDF(tab, direction) {
        const pdfs = resultsPDFs[tab];
        currentImageIndex =
          (currentImageIndex + direction + pdfs.length) % pdfs.length;
        updatePDF(tab);
      }

      const tabs = document.querySelectorAll(".tab-item");
      const indicator = document.querySelector(".tab-indicator");
      let activeTab = 0;

      function updateTabs() {
        tabs.forEach((tab, index) => {
          tab.classList.remove("active");
          if (index === activeTab) {
            tab.classList.add("active");
          }
        });

        const activeTabElement = tabs[activeTab];
        const tabWidth = activeTabElement.offsetWidth;
        const tabPosition = activeTabElement.offsetLeft;
        const tabHeight = activeTabElement.offsetHeight;
        indicator.style.width = `6vw`;
        indicator.style.left = `${tabPosition}px`;
        indicator.style.height = `${tabHeight}px`;
      }

      tabs.forEach((tab, index) => {
        tab.addEventListener("click", () => {
          activeTab = index;
          currentImageIndex = 0;
          updateTabs();
          const activeTabName = tab.dataset.tabName;
          createImageIndicators(resultsPDFs[activeTabName].length); // ÂàõÂª∫ÁôΩÁÇπ
          setTimeout(() => {
            updatePDF(activeTabName);
          }, 200);
        });
      });
      document.getElementById("prev-btn").addEventListener("click", () => {
        const activeTabName = tabs[activeTab].dataset.tabName;
        switchPDF(activeTabName, -1);
      });

      document.getElementById("next-btn").addEventListener("click", () => {
        const activeTabName = tabs[activeTab].dataset.tabName;
        switchPDF(activeTabName, 1);
      });
      function createImageIndicators(count) {
        const indicatorContainer = document.getElementById("img-indicator");
        indicatorContainer.innerHTML = "";

        for (let i = 0; i < count; i++) {
          const dot = document.createElement("div");
          dot.classList.add("indicator-dot");
          if (i === currentImageIndex) dot.classList.add("active");
          indicatorContainer.appendChild(dot);
        }
      }

      // Êõ¥Êñ∞ÊåáÁ§∫Âô®ÁÇπÊ†∑Âºè
      function updateImageIndicators() {
        const dots = document.querySelectorAll(".indicator-dot");
        dots.forEach((dot, index) => {
          dot.classList.toggle("active", index === currentImageIndex);
        });
      }

      // ÂàùÂßãÂåñÊõ¥Êñ∞
      updateTabs();
      createImageIndicators(resultsPDFs["CVACT"].length);
      updatePDF("CVACT");
    </script>

    <script>
      // PDF Êñá‰ª∂Êï∞ÁªÑ
      const datasetsPDFs = [
        "./static/datasets/fig1.pdf",
        "./static/datasets/fig2.pdf",
        "./static/datasets/Fig3.pdf",
      ];

      let datasetsCurrentPDFIndex = 0;

      // ÂºÇÊ≠•Âä†ËΩΩÂíåÊ∏≤Êüì PDF Âà∞ Canvas
      function loaddatasetsPDFAsync(src) {
        const canvas = document.getElementById("datasets-canvas");
        const context = canvas.getContext("2d");

        pdfjsLib.getDocument(src).promise.then((pdf) => {
          // Ëé∑ÂèñÁ¨¨‰∏ÄÈ°µ
          pdf.getPage(1).then((page) => {
            const viewport = page.getViewport({ scale: 1.5 });
            canvas.width = viewport.width;
            canvas.height = viewport.height;
            const renderContext = {
              canvasContext: context,
              viewport: viewport,
            };
            page.render(renderContext);
          });
        });

        // Êõ¥Êñ∞ÂàÜÈ°µÊåáÁ§∫Âô®
        updatedatasetsPagination();
      }

      // Êõ¥Êñ∞ÂàÜÈ°µÊåáÁ§∫Âô®
      function updatedatasetsPagination() {
        const paginationContainer = document.querySelector(
          ".datasets-pagination"
        );
        paginationContainer.innerHTML = ""; // Ê∏ÖÁ©∫ÂéüÊúâÂÜÖÂÆπ

        datasetsPDFs.forEach((_, index) => {
          const dot = document.createElement("div");
          dot.className =
            "dot" + (index === datasetsCurrentPDFIndex ? " active" : "");
          paginationContainer.appendChild(dot);
        });
      }

      // ÂàáÊç¢ PDF Êñá‰ª∂
      function switchdatasetsPDF(direction) {
        datasetsCurrentPDFIndex =
          (datasetsCurrentPDFIndex + direction + datasetsPDFs.length) %
          datasetsPDFs.length;
        loaddatasetsPDFAsync(datasetsPDFs[datasetsCurrentPDFIndex]);
      }

      // ÊåâÈíÆ‰∫ã‰ª∂ÁõëÂê¨
      document
        .getElementById("datasets-prev-btn")
        .addEventListener("click", () => switchdatasetsPDF(-1));
      document
        .getElementById("datasets-next-btn")
        .addEventListener("click", () => switchdatasetsPDF(1));

      // ÂàùÂßãÂåñÂä†ËΩΩÁ¨¨‰∏Ä‰∏™ PDF
      loaddatasetsPDFAsync(datasetsPDFs[datasetsCurrentPDFIndex]);
    </script>
  </body>
</html>
